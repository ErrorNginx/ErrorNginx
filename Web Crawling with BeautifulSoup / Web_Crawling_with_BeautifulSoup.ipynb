{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyebWw5WZ0rZdhLYiReokD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErrorNginx/ErrorNginx/blob/main/Web%20Crawling%20with%20BeautifulSoup%20/%20Web_Crawling_with_BeautifulSoup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvwc64tSQiiT",
        "outputId": "1c262881-c734-4cd3-a807-e44396a21d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trying to export\n",
            "(0, 0)\n",
            "trying to export\n",
            "(0, 0)\n",
            "trying to export\n",
            "(0, 0)\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def get_links(url):\n",
        "    req = requests.get(url)\n",
        "    soup = BeautifulSoup(req.text, \"html5lib\")\n",
        "\n",
        "    #finding paging page\n",
        "    news_links = soup.find_all(\"li\", {'class':'p1520 art-list pos_rel'})\n",
        "    \n",
        "    return news_links\n",
        "\n",
        "def crawling(links):\n",
        "    result = []\n",
        "    for idx, news in enumerate(links):\n",
        "        news_dict = {}\n",
        "        news_title = news.find('a',{'class':'f20 ln24 fbo txt-oev-2'}).text\n",
        "        news_url = news.find('a',{'class':'f20 ln24 fbo txt-oev-2'}).get('href')\n",
        "    \n",
        "        req_news = requests.get(news_url)\n",
        "        soup_news = BeautifulSoup(req_news.text,'html5lib')\n",
        "    \n",
        "    \n",
        "        content_news = soup_news.find('div',{'class':'side-article txt-article'})\n",
        "    \n",
        "        p = content_news.find_all('p')\n",
        "        content = ' '.join(item .text for item in p)\n",
        "        content = content.encode('utf-8','replace')\n",
        "        \n",
        "        news_dict['id'] = idx\n",
        "        news_dict['url'] = news_url\n",
        "        news_dict['title'] = news_title\n",
        "        news_dict['content'] = content\n",
        "\n",
        "        result.append(news_dict)\n",
        "    \n",
        "    return result\n",
        "\n",
        "if __name__== \"__main__\":\n",
        "\tcategories = ['bisnis','sport','lifestyle']\n",
        "\tfor cat in categories:\n",
        "\t\turl = 'https://www.tribunnews.com/'+cat\n",
        "\t\tlinks = get_links(url)\n",
        "    \n",
        "\t\tcontents = crawling(links)\n",
        "\t\tprint('trying to export')\n",
        "\t\tdf = pd.DataFrame(contents)\n",
        "\t\tdf.to_csv(cat+'.csv')\n",
        "\t\tprint(df.shape)"
      ]
    }
  ]
}